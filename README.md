# Deploying this FastAPI app to Railway

Steps to deploy your project to Railway (two common ways):

1) Deploy via GitHub (recommended)

- Commit and push the repository (including `faculty.index` and `faculty_meta.json`) to GitHub.
- In Railway dashboard, create a new project â†’ "Deploy from GitHub" â†’ connect the repo.
- Railway will detect `Procfile` and run the web command.

2) Deploy via Railway CLI

Install the Railway CLI, login, then run:

```bash
railway login
railway init   # follow prompts to create a project
railway up     # deploys the current folder
```

Notes:
- This project expects the data files `faculty.index` and `faculty_meta.json` to be present in the repository root so the app can load them at runtime.
- The app uses `sentence-transformers` which downloads model weights on first run; ensure the environment can reach the internet or pre-download weights if needed.
- Railway will provide a `PORT` environment variable; the `Procfile` uses it for `uvicorn`.
- If using the `Dockerfile`, you can push a container image instead of using the `Procfile`.

Useful commands locally:

```bash
# create virtualenv
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
# run locally (uvicorn will pick an available port)
uvicorn vector_api:app --reload
```
# ğŸ“ Faculty Finder â€“ Data Pipeline Project

## ğŸ“Œ Project Objective
The goal of this project is to build a complete data pipeline that collects, cleans, stores, and serves faculty information from a college website.  
The system is designed to support semantic search, where a student or researcher can type a natural language query like:

> â€œWho is working on sustainable energy and carbon capture?â€

and retrieve relevant faculty members even if those exact keywords are not present in official titles.

This project focuses on:
- Scraping faculty data (names, bios, research interests)
- Cleaning and structuring the data
- Storing it in a relational database
- Making it ready for NLP and vector search applications

---

## ğŸ§  Project Lifecycle

### 1ï¸âƒ£ Ingestion (Scraper)
- Crawls the college faculty directory
- Fetches HTML of individual faculty profile pages
- Extracts:x  x 
  - Name  
  - Biography  
  - Research Interests  
  - Specialization / Teaching / Publications (if available)

### 2ï¸âƒ£ Transformation (Cleaner)
- Removes HTML tags and noisy text
- Handles:
  - Missing bios  
  - Encoding issues  
  - Special characters  
- Converts scraped output into clean structured JSON

### 3ï¸âƒ£ Storage (Database)
- Uses SQLite (`faculty.db`)
- Stores cleaned data in structured tables
- Ensures persistence after script execution

### 4ï¸âƒ£ Serving (Future Scope)
- API layer using FastAPI (planned)
- Endpoints like:
  - `/faculty/{id}`
  - `/all`

---

## ğŸ› ï¸ Technologies Used
- **Python 3.x**
- **Jupyter Notebook**
- **Libraries:**
  - requests  
  - BeautifulSoup  
  - pandas  
  - sqlite3  
  - json  

---

## ğŸ“‚ Project Structure
```text
Faculty_project-/
â”‚
â”œâ”€â”€ main.py
â”‚   â†’ Runs the complete data pipeline: scraping faculty data, cleaning it,
â”‚     and storing it into the SQLite database.
â”‚
â”œâ”€â”€ final.ipynb
â”‚   â†’ Notebook version of the full pipeline for step-by-step execution
â”‚     and demonstration.
â”‚
â”œâ”€â”€ cleaning.ipynb
â”‚   â†’ Performs data cleaning and transformation on the raw scraped JSON
â”‚     (removes HTML tags, handles nulls, fixes formatting).
â”‚
â”œâ”€â”€ json_to_sqlite.py
â”‚   â†’ Takes cleaned JSON data and inserts it into the SQLite database
â”‚     using a defined schema.
â”‚
â”œâ”€â”€ check_db.py
â”‚   â†’ Utility script to verify database contents and check stored records.
â”‚
â”œâ”€â”€ faculty_raw.json
â”‚   â†’ Raw faculty data scraped directly from the website (uncleaned).
â”‚
â”œâ”€â”€ faculty_clean.json
â”‚   â†’ Cleaned and structured faculty data ready for database storage
â”‚     and NLP tasks.
â”‚
â”œâ”€â”€ faculty.db
â”‚   â†’ SQLite database file storing structured faculty information.
â”‚
â”œâ”€â”€ Prompt_Log.txt
â”‚   â†’ Log of LLM prompts and responses used during the project.
â”‚
â””â”€â”€ README.md
    â†’ Project documentation and usage instructions.

